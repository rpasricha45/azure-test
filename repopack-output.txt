This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-07T05:38:24.173Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.github/
  workflows/
    azure-deploy.yml
src/
  config/
    settings.py
  tests/
    config.py
    conftest.py
    test_functions.py
    test_utils.py
  utils/
    logging.py
    storage.py
  exceptions.py
  models.py
  preprocessor.py
  row_grouper.py
.deployment
.gitignore
application.py
function_app.py
gunicorn.conf.py
perfect.py
README.md
requirements.txt
startup.sh

================================================================
Repository Files
================================================================

================
File: .github/workflows/azure-deploy.yml
================
name: Deploy to Azure Web App

on:
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  id-token: write # This is required for requesting the JWT
  contents: read # This is required for actions/checkout

env:
  AZURE_WEBAPP_NAME: rentrollcleaner
  AZURE_WEBAPP_PACKAGE_PATH: '.'
  PYTHON_VERSION: '3.11'

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Create and start virtual environment
      run: |
        python -m venv venv
        source venv/bin/activate

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create required directories and set permissions
      run: |
        mkdir -p data/test
        mkdir -p output
        chmod +x startup.sh

    - name: Create deployment package
      run: |
        zip -r deploy.zip . \
          -x "*.git*" \
          -x "*.github*" \
          -x "*.pytest_cache*" \
          -x "__pycache__/*" \
          -x "*.pyc" \
          -x "venv/*"

    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'

    - name: Deploy to Azure Web App
      uses: azure/webapps-deploy@v2
      with:
        app-name: ${{ env.AZURE_WEBAPP_NAME }}
        package: deploy.zip
        slot-name: 'production'

================
File: src/config/settings.py
================
import os
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Base directory
BASE_DIR = Path(__file__).resolve().parent.parent.parent

# Environment
ENV = os.getenv('ENV', 'development')

# OpenAI settings
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# File paths configuration
class FileConfig:
    TEST_FILES = {
        'arboria': BASE_DIR / 'data' / 'test' / 'Harbor Court September 30, 2019 Rent Roll.xlsx',
        'amazing_grace': BASE_DIR / 'data' / 'test' / '01-2024 TRERA Rent Roll.XLSX',
        'clarendale': BASE_DIR / 'data' / 'test' / 'MM-Rent Roll.xlsx',
        'west_chester': BASE_DIR / 'data' / 'test' / 'West Chester Rent Roll_6.26.19.xlsx'
    }
    OUTPUT_DIR = BASE_DIR / 'output'
    DEFAULT_OUTPUT = 'processed_rent_roll.csv'

    @classmethod
    def setup_directories(cls):
        """Ensure all necessary directories exist"""
        cls.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

================
File: src/tests/config.py
================
from src.config.settings import FileConfig

TEST_CONFIG = {
    'TEST_FILES': {
        name: str(path) for name, path in FileConfig.TEST_FILES.items()
    },
    'OUTPUT_DIR': str(FileConfig.OUTPUT_DIR),
    'DEFAULT_OUTPUT': FileConfig.DEFAULT_OUTPUT
}

def setup_test_env():
    """Create necessary test directories"""
    FileConfig.setup_directories()

================
File: src/tests/conftest.py
================
import pytest

pytest.register_assert_rewrite('src.tests.test_functions')

def pytest_configure(config):
    config.addinivalue_line(
        "markers", "asyncio: mark test as async"
    )

================
File: src/tests/test_functions.py
================
import pytest
import pandas as pd
import os
from pathlib import Path
import sys

# Add the project root to Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from function_app import RentRollProcessor

# Create fixture for test files
@pytest.fixture
def test_files_dir():
    """Fixture to provide path to test files directory"""
    return Path(__file__).parent / "test_files"

@pytest.fixture
def sample_rent_roll(test_files_dir):
    """Fixture to provide path to a sample rent roll file"""
    file_path = test_files_dir / "sample_rent_roll.xlsx"
    if not file_path.exists():
        pytest.skip(f"Test file not found: {file_path}")
    return str(file_path)

class TestRentRollProcessor:
    
    @pytest.mark.asyncio
    async def test_header_detection(self, sample_rent_roll):
        """Test header row detection for each tab."""
        processor = RentRollProcessor(sample_rent_roll)
        
        try:
            # Process the file
            result = await processor.process_file(sample_rent_roll)
            
            # Assert the result contains expected data
            assert result is not None
            # Add more specific assertions based on your expected output
            
        except Exception as e:
            pytest.fail(f"Test failed with exception: {str(e)}")

    @pytest.mark.asyncio
    async def test_row_grouping(self, sample_rent_roll):
        """Test row grouping functionality"""
        processor = RentRollProcessor(sample_rent_roll)
        
        try:
            # Process the file
            result = await processor.process_file(sample_rent_roll)
            
            # Add assertions to verify row grouping
            assert result is not None
            # Add more specific assertions based on your expected output
            
        except Exception as e:
            pytest.fail(f"Test failed with exception: {str(e)}")

================
File: src/tests/test_utils.py
================
import logging
import pandas as pd
from typing import Dict, List
from src.models import RowGroup
from src.row_grouper import RowGrouper
from .config import TEST_CONFIG, setup_test_env
import os

def setup_logging():
    """Configure logging for tests"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

async def process_and_export_data(df: pd.DataFrame, column_mapping: Dict[str, str]):
    """Process data and export results"""
    # Debug: Show all columns
    print("\nAll available columns:")
    for idx, col in enumerate(df.columns):
        print(f"  {idx}: {col}")
    
    print("\nFinal column mapping being used:")
    for key, value in column_mapping.items():
        print(f"  {key}: {value}")
    
    # Process groups
    grouper = RowGrouper()
    groups = grouper.group_rows(df, column_mapping)
    
    print(f"\nFound {len(groups)} row groups")
    
    # Display sample groups
    display_sample_groups(groups, column_mapping)
    
    # Export and show statistics
    await export_to_csv(groups)
    display_statistics(groups)

def display_sample_groups(groups: List[RowGroup], column_mapping: Dict[str, str]):
    """Display sample of first 3 groups"""
    print("\nSample of first 3 groups (with column details):")
    for i, group in enumerate(groups[:3]):
        print_group_info(group, column_mapping, i)

def print_group_info(group: RowGroup, column_mapping: Dict[str, str], index: int):
    """Print formatted group information"""
    print(f"\nGroup {index + 1}:")
    print("Primary row data:")
    for col in column_mapping.values():
        if col in group.primary_row:
            print(f"  {col}: {group.primary_row[col]}")
    
    print(f"Secondary rows: {len(group.secondary_rows)}")
    for j, secondary in enumerate(group.secondary_rows[:2]):
        print(f"  Secondary {j + 1}:")
        for col in column_mapping.values():
            if col in secondary:
                print(f"    {col}: {secondary[col]}")

async def export_to_csv(groups: List[RowGroup]):
    """Export processed data to CSV"""
    setup_test_env()
    csv_data = prepare_csv_data(groups)
    output_path = os.path.join(TEST_CONFIG['OUTPUT_DIR'], TEST_CONFIG['DEFAULT_OUTPUT'])
    
    df_output = pd.DataFrame(csv_data)
    df_output.to_csv(output_path, index=False)
    print(f"\nData exported to: {output_path}")

def display_statistics(groups: List[RowGroup]):
    """Display group statistics"""
    groups_with_secondary = sum(1 for g in groups if g.secondary_rows)
    print("\nGroup Statistics:")
    print(f"Total groups: {len(groups)}")
    print(f"Groups with secondary residents: {groups_with_secondary}")
    if len(groups) > 0:
        avg_secondary = sum(len(g.secondary_rows) for g in groups)/len(groups)
        print(f"Average secondary residents per group: {avg_secondary:.2f}")

def prepare_csv_data(groups: List[RowGroup]) -> List[Dict]:
    """Convert groups to CSV-ready format"""
    csv_data = []
    
    for group in groups:
        # Process primary resident
        primary_data = create_resident_data(group, group.primary_row, True)
        csv_data.append(primary_data)
        
        # Process secondary residents
        for sec_row in group.secondary_rows:
            secondary_data = create_resident_data(group, sec_row, False)
            csv_data.append(secondary_data)
            
    return csv_data

def create_resident_data(group: RowGroup, row: pd.Series, is_primary: bool) -> Dict:
    """Create a data dictionary for a resident"""
    data = {
        'unit_number': group.unit_info.get('number', ''),
        'unit_type': group.unit_info.get('type', ''),
        'unit_rate': group.unit_info.get('rate', ''),
        'primary_resident': is_primary,
    }
    
    # Add non-empty columns from row
    for col in row.index:
        if (col not in data and 
            not str(col).startswith('Unnamed:') and 
            pd.notna(row[col])):
            data[col] = row[col]
            
    return data

================
File: src/utils/logging.py
================
import logging
import sys
from pathlib import Path
from src.config.settings import FileConfig, ENV

def setup_logging(name: str = None) -> logging.Logger:
    """Configure logging with proper formatting and handling"""
    logger = logging.getLogger(name or __name__)
    
    if not logger.handlers:
        logger.setLevel(logging.DEBUG if ENV == 'development' else logging.INFO)
        
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(
            logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        )
        logger.addHandler(console_handler)
        
        # File handler
        log_file = FileConfig.OUTPUT_DIR / 'app.log'
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(
            logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        )
        logger.addHandler(file_handler)
    
    return logger

================
File: src/utils/storage.py
================
from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions
from datetime import datetime, timedelta
import os
import logging

logger = logging.getLogger(__name__)

class AzureStorageClient:
    def __init__(self):
        conn_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
        if not conn_str:
            raise ValueError("Azure Storage connection string not found in environment variables")
        
        self.service_client = BlobServiceClient.from_connection_string(conn_str)
        
    def upload_file(self, file_path: str, blob_name: str, folder: str = None) -> str:
        """Upload a file to Azure Blob Storage in the rentrolls container"""
        try:
            # Always upload to rentrolls container
            blob_client = self.service_client.get_blob_client(
                container="rentrolls",
                blob=blob_name
            )
            
            with open(file_path, "rb") as data:
                blob_client.upload_blob(data, overwrite=True)
                
            return f"rentrolls/{blob_name}"
            
        except Exception as e:
            logger.error(f"Error uploading file to blob storage: {str(e)}")
            raise
            
    def get_download_url(self, blob_path: str) -> str:
        """Generate a SAS URL for downloading a blob from processed container"""
        try:
            # Always get from processed container
            blob_client = self.service_client.get_blob_client(
                container="processed",
                blob=blob_path
            )
            
            # Generate SAS token valid for 24 hours
            sas_token = generate_blob_sas(
                account_name=self.service_client.account_name,
                container_name="processed",
                blob_name=blob_path,
                account_key=self.service_client.credential.account_key,
                permission=BlobSasPermissions(read=True),
                expiry=datetime.utcnow() + timedelta(hours=24)
            )
            
            return f"{blob_client.url}?{sas_token}"
            
        except Exception as e:
            logger.error(f"Error generating download URL: {str(e)}")
            raise

================
File: src/exceptions.py
================
class RentRollError(Exception):
    """Base exception for RentRoll application"""
    pass

class PreprocessorError(RentRollError):
    """Raised when there's an error in preprocessing"""
    pass

class GroupingError(RentRollError):
    """Raised when there's an error in row grouping"""
    pass

class ConfigurationError(RentRollError):
    """Raised when there's a configuration error"""
    pass

================
File: src/models.py
================
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
import pandas as pd

@dataclass
class TabAnalysis:
    """Represents the analysis results of an Excel tab"""
    score: float
    header_row_index: Optional[int]
    matched_patterns: Dict[str, List[str]]
    column_mapping: Optional[Dict[str, str]] = None

@dataclass
class RowGroup:
    """Represents a group of related rows in the rent roll"""
    unit_info: Dict[str, Any]
    primary_row: pd.Series
    secondary_rows: List[pd.Series]

@dataclass
class ProcessingConfig:
    """Configuration for rent roll processing"""
    min_tab_score: int = 25
    header_search_rows: int = 20
    column_patterns: Dict[str, List[str]] = None
    pattern_weights: Dict[str, int] = None
    min_header_score: int = 4

    def __post_init__(self):
        if self.column_patterns is None:
            self.column_patterns = {
                'unit': ['unit', 'apt', 'room', 'apartment', 'number', 'suite'],
                'resident': ['resident', 'tenant', 'name', 'occupant'],
                'rate': ['rate', 'rent', 'charge', 'fee', 'payment'],
                'date': ['move', 'date', 'admission'],
                'care': ['care', 'level', 'service']
            }
        
        if self.pattern_weights is None:
            self.pattern_weights = {
                'unit': 10,
                'resident': 10,
                'rate': 10,
                'date': 5,
                'care': 5
            }

================
File: src/preprocessor.py
================
import pandas as pd
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from openai import AsyncOpenAI
import json
import os

@dataclass
class TabAnalysis:
    score: float
    header_row_index: Optional[int]
    matched_patterns: Dict[str, List[str]]
    column_mapping: Optional[Dict[str, str]] = None

@dataclass
class RowGroup:
    unit_info: Dict[str, Any]
    primary_row: pd.Series
    secondary_rows: List[pd.Series]

class RentRollPreprocessor:
    def __init__(self, excel_path: str):
        self.excel_path = str(excel_path).replace('\\', '/')
        self.config = self._get_default_config()
        self.logger = logging.getLogger(__name__)
        self.workbook = None
        self.tab_analyses = {}
        self.client = AsyncOpenAI()
        
        try:
            self.workbook = pd.ExcelFile(self.excel_path)
        except Exception as e:
            self.logger.error(f"Failed to load Excel file: {str(e)}")
            raise

    def _get_default_config(self) -> Dict:
        return {
            'min_tab_score': 25,
            'header_search_rows': 20,
            'column_patterns': {
                'unit': ['unit', 'apt', 'room', 'apartment', 'number', 'suite'],
                'resident': ['resident', 'tenant', 'name', 'occupant'],
                'rate': ['rate', 'rent', 'charge', 'fee', 'payment'],
                'date': ['move', 'date', 'admission'],
                'care': ['care', 'level', 'service']
            },
            'pattern_weights': {
                'unit': 10,
                'resident': 10,
                'rate': 10,
                'date': 5,
                'care': 5
            },
            'min_header_score': 4
        }

    async def analyze_tabs(self) -> Dict[str, TabAnalysis]:
        """Analyze all tabs in the workbook."""
        self.tab_analyses = {}
        
        for sheet_name in self.workbook.sheet_names:
            try:
                df = pd.read_excel(self.workbook, sheet_name, header=None, nrows=None, skiprows=0)
                analysis = await self._analyze_single_tab(df)
                
                if analysis.header_row_index is not None:
                    clean_headers = self.get_clean_headers(df, analysis.header_row_index)
                    df_with_header = pd.read_excel(
                        self.workbook, 
                        sheet_name, 
                        header=analysis.header_row_index
                    )
                    analysis.column_mapping = await self.map_columns_with_ai(clean_headers, df_with_header)
                
                self.tab_analyses[sheet_name] = analysis
                
            except Exception as e:
                self.logger.error(f"Error analyzing tab {sheet_name}: {str(e)}")
                continue
                
        return self.tab_analyses

    async def _analyze_single_tab(self, df: pd.DataFrame) -> TabAnalysis:
        score = 0
        header_row_index = None
        matched_patterns = {pattern: [] for pattern in self.config['column_patterns'].keys()}

        try:
            df = df.astype(str).replace('nan', '')
            header_row_index = self._find_header_row(df)
            
            if header_row_index is not None:
                headers = df.iloc[header_row_index].astype(str).str.lower()
                
                for col, header in enumerate(headers):
                    for pattern_type, patterns in self.config['column_patterns'].items():
                        if any(pattern in header for pattern in patterns):
                            weight = self.config['pattern_weights'][pattern_type]
                            score += weight
                            matched_patterns[pattern_type].append(header)

            return TabAnalysis(score=score, header_row_index=header_row_index, matched_patterns=matched_patterns, column_mapping=None)
                
        except Exception as e:
            self.logger.error(f"Error in _analyze_single_tab: {str(e)}")
            return TabAnalysis(score=0, header_row_index=None, matched_patterns=matched_patterns, column_mapping=None)

    def _find_header_row(self, df: pd.DataFrame) -> Optional[int]:
        max_score = 0
        best_row = None
        
        for idx in range(min(self.config['header_search_rows'], len(df))):
            row = df.iloc[idx].astype(str).str.lower()
            score = 0
            
            for pattern_type, patterns in self.config['column_patterns'].items():
                if any(row.str.contains(pattern).any() for pattern in patterns):
                    score += self.config['pattern_weights'][pattern_type] // 5
                    
            if row.str.match(r'^\$?\d+\.?\d*$').any():
                score -= 2
                
            if score > max_score:
                max_score = score
                best_row = idx
                
        return best_row if max_score >= self.config['min_header_score'] else None

    def get_clean_headers(self, df: pd.DataFrame, header_row_index: int) -> List[str]:
        """Get non-blank headers from the detected header row."""
        headers = df.iloc[header_row_index].astype(str)
        # Filter out blank or NaN headers
        return [h for h in headers if h and h.strip() and h.lower() != 'nan']

    async def map_columns_with_ai(self, headers: List[str], df: pd.DataFrame) -> Dict[str, str]:
        """Use OpenAI to intelligently map column headers and sample data."""
        print("\nStarting AI column mapping...")
        
        clean_headers = [str(h).strip() for h in headers if h]
        print(f"Clean headers: {clean_headers}")
        
        # Get first 3 rows of data as examples, using column indices instead of names
        sample_data = []
        df_columns = df.columns.tolist()
        for _, row in df.head(3).iterrows():
            row_data = {}
            for header in clean_headers:
                # Find the closest matching column name
                matching_col = next(
                    (col for col in df_columns if str(col).strip() == header),
                    None
                )
                if matching_col is not None:
                    row_data[header] = str(row[matching_col]).strip()
                else:
                    print(f"Warning: Could not find matching column for header: {header}")
                    row_data[header] = "N/A"
            sample_data.append(row_data)
        
        print(f"\nSample data prepared: {json.dumps(sample_data, indent=2)}")
        
        prompt = f"""Given these column headers and sample data from a senior living rent roll:

Headers:
{clean_headers}

Sample Data (first 3 rows):
{json.dumps(sample_data, indent=2)}

Map the columns to these standard senior living categories, analyzing both header names AND sample data values:

1. unit: Column containing unique unit/room/apartment identifiers
   - Look for: Room numbers, unit IDs, apartment numbers 
   - Usually short alphanumeric codes (e.g., "101A", "2B", "U203")
   - Should have low repetition (typically <3 times in sample)
   - Example headers: RoomNumber, Unit, AptNum, UnitID

2. resident: Column containing resident names
   - Look for: Full names, typically First/Last name combinations
   - May include titles (Mr., Mrs., etc.)
   - Example headers: ResidentName, Tenant, OccupantName, Name

3. rate: Column containing monthly rent/rate amounts 
   - Look for: Numeric values with typical senior living price ranges ($2000-$10000)
   - May include currency symbols ($) or decimal points
   - Example headers: MonthlyRate, RentAmount, BaseRate, Rate

4. type: Column indicating level of care
   - Look for these specific values:
     - IL (Independent Living)
     - AL (Assisted Living)
     - MC (Memory Care)
     - NC (Nursing Care)
     - EAL (Enhanced Assisted Living)
   - Example headers: CareType, ResidentType, LevelOfCare, ServiceType, Type

5. date: Column containing resident move-in dates
   - Look for: Date formats (YYYY-MM-DD, MM/DD/YYYY, etc.)
   - Should contain valid calendar dates
   - Example headers: MoveInDate, AdmissionDT, StartDate, ResidencyDate

Rules:
- Return exactly ONE best matching column for each category
- Only use column headers that exist in the provided data
- If no good match exists for a category, set it to null
- Base matches on both header names AND actual data content
- Prioritize accuracy over completeness

Return a JSON object in this exact format:
{{
    "unit": "exact_matching_column_name",
    "resident": "exact_matching_column_name",
    "rate": "exact_matching_column_name", 
    "type": "exact_matching_column_name",
    "date": "exact_matching_column_name"
}}
"""

        print(f"\nPrompt being sent to OpenAI:\n{prompt}")

        try:
            print("\nCalling OpenAI API...")
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": "You are a specialized assistant that maps rent roll columns exactly. Analyze both header names and data content. Return only the JSON object, no markdown formatting."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                temperature=0
            )

            print("\nReceived response from OpenAI")
            raw_response = response.choices[0].message.content
            print(f"\nAI Response Raw: {raw_response}")

            # Clean and parse the response
            cleaned_response = raw_response.strip()
            if cleaned_response.startswith('```'):
                cleaned_response = cleaned_response.split('\n', 1)[1]
                cleaned_response = cleaned_response.rsplit('\n', 1)[0]
                cleaned_response = cleaned_response.replace('```json', '').replace('```', '').strip()

            print(f"Cleaned response: {cleaned_response}")

            try:
                mapping = json.loads(cleaned_response)
                print(f"Parsed mapping: {mapping}")
            except json.JSONDecodeError as e:
                print(f"JSON parsing error: {e}")
                return {}

            # Validate the mapping and find exact column matches
            valid_categories = {'unit', 'resident', 'rate', 'type', 'date'}
            df_columns = df.columns.tolist()
            validated_mapping = {}
            
            for k, v in mapping.items():
                if k in valid_categories:
                    if v is None:
                        # Keep the null value in the mapping
                        validated_mapping[k] = None
                    else:
                        # Find the exact column name from DataFrame that best matches the AI suggestion
                        matching_col = next(
                            (col for col in df_columns if str(col).strip() == str(v).strip()),
                            None
                        )
                        validated_mapping[k] = matching_col
            
            print(f"Final validated mapping: {validated_mapping}")
            return validated_mapping

        except Exception as e:
            self.logger.error(f"Error in AI column mapping: {str(e)}")
            print(f"\nFull error in AI mapping: {str(e)}")
            print("Falling back to rule-based mapping...")
            # Fall back to rule-based mapping
            return self._create_column_mapping(headers)

    def _create_column_mapping(self, columns) -> Dict[str, str]:
        column_mapping = {}
        columns_lower = [str(col).lower() for col in columns]
        
        for pattern_type, patterns in self.config['column_patterns'].items():
            for col, col_lower in zip(columns, columns_lower):
                if any(pattern in col_lower for pattern in patterns):
                    column_mapping[pattern_type] = col
                    break
        
        return column_mapping 

    async def extract_property_info(self) -> Tuple[str, str]:
        """Extract property name and as of date from the file and its contents."""
        print("\nExtracting property info...")
        
        # Get filename without extension for fallback
        file_name = os.path.basename(self.excel_path)
        base_name = os.path.splitext(file_name)[0]
        
        # Collect data from first 20 rows of each tab
        tab_data = []
        for sheet_name in self.workbook.sheet_names:
            try:
                df = pd.read_excel(self.workbook, sheet_name, header=None, nrows=20)
                # Convert to string and replace NaN/empty cells with empty string
                cleaned_rows = []
                for _, row in df.iterrows():
                    cleaned_row = []
                    for cell in row:
                        # Skip empty or NaN cells
                        if pd.isna(cell) or str(cell).strip() == '':
                            continue
                        cleaned_row.append(str(cell).strip())
                    if cleaned_row:  # Only add rows that have content
                        cleaned_rows.append(cleaned_row)
                
                if cleaned_rows:  # Only add tabs that have content
                    tab_data.append({
                        'tab_name': sheet_name,
                        'rows': cleaned_rows
                    })
            except Exception as e:
                self.logger.error(f"Error reading tab {sheet_name}: {str(e)}")
                continue

        prompt = f"""Analyze this information from a senior living rent roll Excel file to identify the property name and as of date:

File name: {base_name}

Tab data from first 20 rows of each sheet:
{json.dumps(tab_data, indent=2)}

Rules for property name:
- Look for phrases like "Property:", "Community:", "Facility:"
- Common locations: file name, sheet names, top rows of sheets
- Should be a real property name (e.g., "Heartis Peoria", "Harbor Court")
- Ignore generic text like "Rent Roll", "Report"

Rules for as of date:
- Look for phrases like "As of", "As of Period:", "Period:", "For Month Of"
- Common date formats: MM/DD/YYYY, YYYY-MM-DD, Month DD YYYY
- Focus on most recent/relevant date
- Ignore future dates or move-in dates

Return a JSON object in this exact format:
{{
    "property_name": "extracted property name",
    "as_of_date": "extracted date in MM-DD-YYYY format"
}}

If either value cannot be determined with confidence, use null."""

        print("\nData being sent to OpenAI:")
        print("=" * 80)
        print(f"File name: {base_name}")
        print("\nTab data:")
        print(json.dumps(tab_data, indent=2))
        print("=" * 80)

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "You are a specialized assistant that extracts property names and dates from rent roll files. Return only the JSON object, no additional text."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0
            )

            print("\nOpenAI Response:")
            print(response.choices[0].message.content)

            result = json.loads(response.choices[0].message.content)
            
            # Use original filename if AI extraction fails
            property_name = result.get('property_name')
            as_of_date = result.get('as_of_date')
            
            if not property_name or not as_of_date:
                print(f"\nFalling back to original filename: {base_name}")
                return base_name, ""  # Empty string for date when using filename
                
            return property_name, as_of_date

        except Exception as e:
            self.logger.error(f"Error extracting property info: {str(e)}")
            print(f"\nFalling back to original filename due to error: {base_name}")
            return base_name, ""  # Empty string for date when using filename

================
File: src/row_grouper.py
================
import pandas as pd
from typing import List, Dict, Any, Optional
import logging
from src.models import RowGroup
from src.utils.logging import setup_logging

class RowGrouper:
    def __init__(self):
        self.logger = setup_logging(__name__)
        self.config = {
            'primary_row_indicators': {
                'required_fields': ['unit', 'rate']
            },
            'grouping_fields': {
                'unit_number': ['unit', 'apt', 'room', 'apartment', 'apt#', 'unit#']
            }
        }
    
    def get_unit_info(self, row: pd.Series, column_mapping: Dict[str, str]) -> Dict[str, Any]:
        unit_fields = self.config['grouping_fields']['unit_number']
        unit_number = []
        
        # Get the unit number from mapped column if it exists
        if 'unit' in column_mapping and column_mapping['unit'] is not None:
            unit_value = row[column_mapping['unit']]
            if pd.notna(unit_value):
                unit_number.append(str(unit_value))
        
        # Build unit info dict with safe handling of None values in mapping
        unit_info = {
            'number': '-'.join(unit_number) if unit_number else None,
            'type': str(row[column_mapping['type']]) if 'type' in column_mapping and column_mapping['type'] is not None else '',
            'rate': str(row[column_mapping['rate']]) if 'rate' in column_mapping and column_mapping['rate'] is not None else '',
            'resident': str(row[column_mapping['resident']]) if 'resident' in column_mapping and column_mapping['resident'] is not None else '',
            'move_in_date': str(row[column_mapping['date']]) if 'date' in column_mapping and column_mapping['date'] is not None else ''
        }
        
        return unit_info
        
    def group_rows(self, df: pd.DataFrame, column_mapping: Dict[str, str]) -> List[RowGroup]:
        groups = []
        current_group = None
        current_unit_number = None
        
        def should_start_new_group(row: pd.Series, mapping: Dict[str, str]) -> bool:
            """Check if this row should start a new group"""
            # If it has a unit number and either rate or resident info, it's a primary row
            unit_field = mapping.get('unit')
            rate_field = mapping.get('rate')
            resident_field = mapping.get('resident')
            
            has_unit = unit_field and pd.notna(row[unit_field])
            has_rate = rate_field and pd.notna(row[rate_field])
            has_resident = resident_field and pd.notna(row[resident_field])
            
            return has_unit and (has_rate or has_resident)
        
        for idx, row in df.iterrows():
            unit_info = self.get_unit_info(row, column_mapping)
            unit_number = unit_info['number']
            
            # Start new group if this looks like a primary row
            if should_start_new_group(row, column_mapping):
                if current_group is not None:
                    groups.append(current_group)
                
                current_group = RowGroup(
                    unit_info=unit_info,
                    primary_row=row,
                    secondary_rows=[]
                )
                current_unit_number = unit_number
            
            # Add as secondary row if we have a current group
            elif current_group is not None:
                # Don't filter out rows, just add them as secondary
                current_group.secondary_rows.append(row)
        
        # Don't forget the last group
        if current_group is not None:
            groups.append(current_group)
        
        return groups

================
File: .deployment
================
[config]
SCM_DO_BUILD_DURING_DEPLOYMENT=true

================
File: .gitignore
================
# Local development settings
local.settings.json
.env
repopack-output.txt

# Python
__pycache__/
*.py[cod]
*$py.class
.Python
venv/
ENV/

# VS Code
.vscode/

# Azure Functions artifacts
bin
obj
appsettings.json
.python_packages

# Azurite artifacts
__azurite*
__blobstorage__
__queuestorage__
__tablestorage__

# Data and output directories
data/
output/
*.csv
*.xlsx
*.xls
*.pdf

# Debug logs
*.log
c:azuritedebug.log

# Mac OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes

================
File: application.py
================
from flask import Flask, request, jsonify
from perfect import RentRollProcessor
import asyncio
import tempfile
import os
from src.utils.storage import AzureStorageClient
from src.utils.logging import setup_logging

app = Flask(__name__)
logger = setup_logging()
storage_client = AzureStorageClient()

with app.app_context():
    logger.info("Initializing application...")

@app.route("/")
def home():
    logger.info("Home route accessed")
    return jsonify({
        "status": "online",
        "message": "Rent Roll Processor API is running"
    })
@app.route("/health")
def health():
    return jsonify({
        "status": "healthy"
    })

@app.route("/api/process", methods=["POST"])
async def process_rent_roll():
    if "file" not in request.files:
        return jsonify({"error": "No file part"}), 400
    
    file = request.files["file"]
    if file.filename == "":
        return jsonify({"error": "No selected file"}), 400
        
    try:
        # Create a temporary file for initial upload
        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as temp_file:
            file.save(temp_file.name)
            
            # Upload to rentrolls container
            upload_path = storage_client.upload_file(
                temp_file.name,
                file.filename
            )
            logger.info(f"File uploaded to blob storage: {upload_path}")
            
            # Process the file
            await RentRollProcessor.process_file(temp_file.name)
            
            # Get the output filename
            output_filename = os.path.splitext(file.filename)[0] + ".csv"
            output_path = os.path.join("output", output_filename)
            
            # Check if output file exists
            if os.path.exists(output_path):
                # Generate download URL from processed container
                download_url = storage_client.get_download_url(output_filename)
                
                return jsonify({
                    "status": "success",
                    "message": "File processed successfully",
                    "download_url": download_url
                })
            else:
                return jsonify({
                    "error": "Processing failed - output file not found"
                }), 500
                
    except Exception as e:
        logger.error(f"Error processing file: {str(e)}")
        return jsonify({
            "error": str(e)
        }), 500
    finally:
        # Cleanup temporary files
        if "temp_file" in locals():
            try:
                os.unlink(temp_file.name)
            except:
                pass
        if os.path.exists(output_path):
            try:
                os.unlink(output_path)
            except:
                pass

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)

================
File: function_app.py
================
import logging
import json
import tempfile
import os
import asyncio
import azure.functions as func
from app import RentRollProcessor

# Add version info
__version__ = "1.0.0"

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def get_storage_connection():
    """Verify storage connection settings"""
    connection = os.environ.get("AzureWebJobsStorage")
    if not connection:
        raise ValueError("AzureWebJobsStorage connection string not found in environment variables")
    logging.info("Storage connection string verified")
    return connection

async def process_rent_roll_blob(blob_data: bytes, filename: str) -> str:
    """Process rent roll data from a blob and return the output path"""
    os.makedirs('output', exist_ok=True)
    
    temp_path = None
    try:
        # Save to temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as temp_file:
            temp_file.write(blob_data)
            temp_path = temp_file.name
        
        # Process the file
        await RentRollProcessor.process_file(temp_path)
        
        # Get output path
        output_filename = os.path.splitext(filename)[0] + '.csv'
        return os.path.join('output', output_filename)
        
    finally:
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
            except Exception as e:
                logger.warning(f"Failed to delete temporary file {temp_path}: {str(e)}")

app = func.FunctionApp()

@app.function_name(name="RentRollProcessor")
@app.blob_trigger(arg_name="blob", 
                 path="rentrolls/{name}",
                 connection="AzureWebJobsStorage")
@app.blob_output(arg_name="outputblob",
                path="processed/{name}.csv",
                connection="AzureWebJobsStorage")
async def process_rent_roll(blob: func.InputStream, outputblob: func.Out[str]) -> None:
    # Verify storage connection at startup
    get_storage_connection()
    
    try:
        logging.info(f"Processing blob: {blob.name}")
        
        # Read blob data
        blob_data = blob.read()
        
        # Process file
        output_path = await process_rent_roll_blob(blob_data, blob.name)
        
        # Write to output blob
        if os.path.exists(output_path):
            with open(output_path, 'r') as f:
                outputblob.set(f.read())
            logging.info(f"Successfully processed {blob.name}")
        else:
            raise FileNotFoundError(f"Output file not found: {output_path}")
        
    except Exception as e:
        logging.error(f"Error processing {blob.name}: {str(e)}")
        raise

================
File: gunicorn.conf.py
================
import multiprocessing

bind = "0.0.0.0:8000"
workers = multiprocessing.cpu_count() * 2 + 1
timeout = 600

================
File: perfect.py
================
import pandas as pd
import os
from src.preprocessor import RentRollPreprocessor
from src.row_grouper import RowGrouper
from src.utils.logging import setup_logging
import asyncio

logger = setup_logging()

class RentRollProcessor:
    @staticmethod
    async def analyze_headers(file_path: str):
        """Analyze header rows for each tab."""
        processor = RentRollPreprocessor(file_path)
        
        for sheet_name in processor.workbook.sheet_names:
            print(f"\nAnalyzing Tab: {sheet_name}")
            print("=" * 60)
            
            try:
                df = pd.read_excel(processor.workbook, sheet_name,
                                 header=None, 
                                 nrows=None,
                                 skiprows=0)
                
                print("\nFirst 5 rows:")
                print(df.head().to_string())
                
                analysis = await processor._analyze_single_tab(df)
                print(f"\nScore: {analysis.score}")
                print(f"Header Row Index: {analysis.header_row_index}")
                
                if analysis.header_row_index is not None:
                    print("\nMatched Patterns:")
                    for pattern_type, matches in analysis.matched_patterns.items():
                        if matches:
                            print(f"{pattern_type}: {matches}")
                
            except Exception as e:
                logger.error(f"Error analyzing tab {sheet_name}: {str(e)}")

    @staticmethod
    async def process_rent_roll(file_path: str):
        """Process rent roll data and group related rows."""
        processor = RentRollPreprocessor(file_path)
        
        print("\nProcessing Rent Roll")
        print("=" * 80)
        
        try:
            # Initialize variables to track best tab
            best_score = -1
            best_tab_name = None
            best_analysis = None

            # Analyze each tab
            for sheet_name in processor.workbook.sheet_names:
                print(f"\nAnalyzing Tab: {sheet_name}")
                
                df = pd.read_excel(processor.workbook, sheet_name,
                                 header=None, 
                                 nrows=None,
                                 skiprows=0)
                
                analysis = await processor._analyze_single_tab(df)
                print(f"Score: {analysis.score}")
                print(f"Column Mapping: {analysis.column_mapping}")
                print(f"Matched Patterns: {analysis.matched_patterns}")
                
                if analysis.score > best_score:
                    best_score = analysis.score
                    best_tab_name = sheet_name
                    best_analysis = analysis
                    print(f"New best tab found: {sheet_name} with score {best_score}")

            if not best_tab_name or not best_analysis:
                print("\nERROR: No valid tabs were found")
                return

            print(f"\nBest tab analysis details:")
            print(f"Tab: {best_tab_name}")
            print(f"Score: {best_score}")
            print(f"Header Row: {best_analysis.header_row_index}")
            print(f"Column Mapping: {best_analysis.column_mapping}")
            print(f"Matched Patterns: {best_analysis.matched_patterns}")

            # Generate column mapping if needed
            if best_analysis.column_mapping is None and best_analysis.matched_patterns:
                print("\nTrying to generate column mapping from matched patterns...")
                
                # First read without headers to get the header row
                df_raw = pd.read_excel(
                    processor.workbook,
                    best_tab_name,
                    header=None
                )
                # Get clean headers from the raw DataFrame
                clean_headers = processor.get_clean_headers(df_raw, best_analysis.header_row_index)
                
                # Then read again with proper headers for the data
                df = pd.read_excel(
                    processor.workbook,
                    best_tab_name,
                    header=best_analysis.header_row_index
                )
                
                # Try AI mapping first
                best_analysis.column_mapping = await processor.map_columns_with_ai(clean_headers, df)
                
                # Fall back to rule-based if AI mapping fails
                if not best_analysis.column_mapping:
                    print("\nFalling back to rule-based mapping...")
                    best_analysis.column_mapping = processor._create_column_mapping(df)
                
                print(f"Generated Column Mapping: {best_analysis.column_mapping}")

            if not best_analysis.column_mapping:
                print("\nERROR: Could not generate valid column mapping")
                print("Available columns in DataFrame:")
                print(df.columns.tolist())
                print("\nMatched Patterns:")
                print(best_analysis.matched_patterns)
                return

            # Process the best tab with the header row we found
            df = pd.read_excel(
                processor.workbook,
                best_tab_name,
                header=best_analysis.header_row_index
            )
            
            print("\nAll available columns:")
            for idx, col in enumerate(df.columns):
                print(f"  {idx}: {col}")
            
            # Get property info and create output filename
            property_name, as_of_date = await processor.extract_property_info()
            if as_of_date:  # If we have a date, use the property name - date format
                safe_filename = f"{property_name} - {as_of_date}".replace('/', '-').replace('\\', '-')
            else:  # Otherwise just use the property name (which would be the original filename)
                safe_filename = property_name.replace('/', '-').replace('\\', '-')
                
            output_path = os.path.join('output', f"{safe_filename}.csv")
            
            os.makedirs('output', exist_ok=True)  # Ensure output directory exists
            
            # Prepare data for export
            export_data = []
            row_grouper = RowGrouper()
            groups = row_grouper.group_rows(df, best_analysis.column_mapping)
            
            for group in groups:
                # Add primary row
                row_data = {
                    'unit_number': group.unit_info['number'],
                    'unit_type': group.unit_info['type'],
                    'rate': group.unit_info['rate'],
                    'resident': group.unit_info['resident'],
                    'move_in_date': group.unit_info['move_in_date'],
                    'is_primary': True
                }
                # Add non-empty columns from primary row
                for col in group.primary_row.index:
                    if (str(col) not in row_data and 
                        pd.notna(group.primary_row[col]) and 
                        str(group.primary_row[col]).strip()):
                        row_data[str(col)] = group.primary_row[col]
                export_data.append(row_data)
                
                # Add secondary rows
                for sec_row in group.secondary_rows:
                    sec_data = {
                        'unit_number': group.unit_info['number'],
                        'unit_type': group.unit_info['type'],
                        'rate': group.unit_info['rate'],
                        'resident': group.unit_info['resident'],
                        'move_in_date': group.unit_info['move_in_date'],
                        'is_primary': False
                    }
                    # Add non-empty columns from secondary row
                    for col in sec_row.index:
                        if (str(col) not in sec_data and 
                            pd.notna(sec_row[col]) and 
                            str(sec_row[col]).strip()):
                            sec_data[str(col)] = sec_row[col]
                    export_data.append(sec_data)
            
            # Export to CSV
            if export_data:
                try:
                    df_export = pd.DataFrame.from_records(export_data)
                    
                    # Drop any columns that are entirely empty
                    df_export = df_export.dropna(axis=1, how='all')
                    
                    # Safer way to handle column dropping
                    columns_to_drop = []
                    for col in df_export.columns:
                        series = df_export.get(col)
                        if series is not None and isinstance(series, pd.Series):
                            if series.astype(str).str.strip().eq('').all():
                                columns_to_drop.append(col)
                    
                    if columns_to_drop:
                        df_export = df_export.drop(columns=columns_to_drop)
                    
                    df_export.to_csv(output_path, index=False)
                    print(f"\nData exported to: {output_path}")
                except Exception as e:
                    logger.error(f"Error creating DataFrame: {str(e)}")
                    raise
            else:
                print("\nNo data to export")
            
        except Exception as e:
            logger.error(f"Error in processing: {str(e)}")
            import traceback
            print(f"\nFull error traceback:")
            print(traceback.format_exc())
            raise

    @staticmethod
    async def process_file(file_path: str):
        """Process a rent roll file."""
        if not os.path.exists(file_path):
            logger.error(f"File not found: {file_path}")
            return
            
        print("\nAnalyzing Headers...")
        print("=" * 80)
        await RentRollProcessor.analyze_headers(file_path)
            
        print("\nProcessing Rent Roll Data...")
        print("=" * 80)
        await RentRollProcessor.process_rent_roll(file_path)

if __name__ == "__main__":
    file_path = "data/test/OLA-Census-06-30-21.xlsx"
    asyncio.run(RentRollProcessor.process_file(file_path))

================
File: README.md
================
# Rent Roll Processor

A robust Python application for processing and analyzing rent roll data from various Excel formats. This tool automatically detects headers, maps columns, and groups related tenant information for structured output.

##  Features

- Automated header detection across multiple Excel tabs
- Intelligent column mapping using AI and rule-based approaches
- Tenant grouping for primary and secondary residents
- Configurable output formats
- Comprehensive testing suite
- Asynchronous processing support

##  Installation

1. Clone the repository:

================
File: requirements.txt
================
# Azure Functions dependencies
azure-functions>=1.15.0
azure-storage-blob>=12.19.0  # Updated from 12.9.0
opencensus-ext-azure>=1.1.9
azure-identity>=1.13.0

# Data processing
pandas>=2.0.0
numpy>=1.24.0
openpyxl>=3.1.2  # Updated from 3.0.9

# Environment variables
python-dotenv>=1.0.0  # Updated from 0.19.0

# Development dependencies
pytest>=7.4.0
pytest-asyncio>=0.21.0
black>=23.3.0
flake8>=6.0.0
openai>=1.3.0

# Web server
flask>=3.0.0
gunicorn>=21.2.0  # Updated from 20.1.0
werkzeug>=3.0.1

================
File: startup.sh
================
#!/bin/bash

# Navigate to the application directory
cd /home/site/wwwroot

# Clean up any existing packages
echo "Cleaning up existing packages..."
pip freeze | xargs pip uninstall -y

# Upgrade pip and install requirements
echo "Upgrading pip..."
python -m pip install --upgrade pip
echo "Installing requirements..."
pip install -r requirements.txt

# Create necessary directories
echo "Creating directories..."
mkdir -p data/test
mkdir -p output

# Start the Flask application with gunicorn
echo "Starting application..."
gunicorn --bind=0.0.0.0:8000 --timeout 600 application:app
